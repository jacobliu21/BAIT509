1. Bagging is a special case of random forests under which case?

Bagging is a special case of random forests under the case when there is only one predictor or m=p.

2. What are the hyperparameters we can control for random forests?

In random forests method, it restricts the choice of choosing predictors to some random subset of $m$ predictors out of the $p$ instead of choosing from the whole set of predictors. 
Therefore, the method of choosing subsets and the number of m could be the hyperparameters.

3. Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?

(1,0), (1,2), (1,5)
(1,2), (2,0)
(1,2), (1,2), (1,5)

(1,2), (2,0) and (1,2), (1,2), (1,5) are valid boostrapped datasets since the observations inside them are chosen or duplicated from the observations in the original dataset. 

4. For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)?

IN (1,2), (2,0) OOB are  (2,0)
IN (1,2), (1,2), (1,5) OOB are (1,2), (1,5)

5. You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases?
Regression: your trees make the following four predictions: 1,1,3,3. 2
Classification: your trees make the following four predictions: "A", "A", "B", "C". A

Answer:
1. When m=p
2. Number of trees/m
3. (1,2), (1,2), (1,5)/ Sample size matters, should be all equal
4. (2,0)
5. 2 and A
